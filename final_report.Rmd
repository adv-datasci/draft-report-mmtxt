---
title: "Hurricane Harvey in America "
author: xueting tao
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# introduction

Hurricane Harvey is an Atlantic hurricane that has attack America for several times in the past year. The most recent attack happened on August 13 when the National Hurricane center detect the hurricane on the western east of Africa. It hit United States on Texas and Louisiana on August 23rd. 
In this project, I am going to use the information from twitter to identify possible the most serious hurt area and its possible moving route.


#Method:

1.	Basic assumption:

1.1.	The severity of the hurricane affects the times people tweets.

1.2.	The location information in the location column the tweet original display is accurate and represent the people’s real location.

1.3.	The missing and useless data is random.

2.	Data extraction:


2.1 The data comes from twitter from Sep 4th to Sep 8th. For each day, I pooled the 10,000 tweets data from the API(except for Sep 4th ) and use the location information as the basic data source to identify the path of hurricane and use the number of the same location occur as the severity of the hit.

2.2 Follow the instruction, first I need to create a Twitter Aplication:

I create a new application with the name of xtao8 and get the API key, API secreat, Access token and Access secret.

2.3 Install twitterR. Since I would like to access Harvey and it mainly start on August 17th at night, I would like to start my search from Sep 4th and follow until Sep 8th. 
If the data set is too big, it would be difficult to analysis. So I would like to pull the first 10,000 sample several times and get useful information from them.

The code I used:

```{r eval=FALSE}
#getting the data: install twitterR since install.pacakge("twitterR")is not avialble
#let me try another one
install.packages(c("devtools", "rjson", "bit64", "httr"))
#follow the instruction and restart R
library(devtools)
install_github("geoffjentry/twitteR")

library(twitteR)
setup_twitter_oauth("gLvw86UAzsVGG4lNoBTCttQjg", 
                    "gbBf9OW1PMz2KXMZx9Wo3neKgXNSpvG3abgt3A3TxcBWxefmjm",
                    "770639700839333888-foZXS5jtNXc7bWdpNnUR18Gb5fPQSp9",
                    "n8lL1J7HlAhupOGdOVJFyLcNObxWtyxkffTGAnytyqjY1")
#selection:
1
```


3. getting and cleaning data:

3.1. First, I would like to try to pull 10000 sample from the twitter randomly and see how much data can be used.
The day I can go back as early as possible is 9/5, so start from 9/5, I will get 5 days data and change it into data frame. The code used list below, using 9-4 and 9-29 as an example

```{r eval=FALSE}
#data at 9-4
sample_9_4<-searchTwitter('#HurricaneHarvey until:2017-9-5', n=10000)
HH9_4<-twListToDF(sample_9_4)
#data at 9-29
HH9_29<-searchTwitter('a until:2017-9-30 ',n=10000)
HH9_29<-twListToDF(HH9_29)

```

3.2. Delete lines without location information in the original dataset and create new dataset called cHHs.(9-4as an example)

```{r eval=FALSE}

#load tidytext and dplyr
library(tidytext)
library(dplyr)

#select data that have location information

HH9_4$location[HH9_4$location==""]<-"NA"  
HH9_4$location<-as.factor(HH9_4$location)
cHH9_4<-filter(HH9_4,location!="NA")

```


3.3 In order to continue analysis, I need first clean the format of the data, seperate the location information by city and state, showing all the charachers in upper form and cleaning the spaces. After that, I seperate the dataset into two part, the one with useful state information and the one without useful state information.

```{r eval=FALSE}

cHH9_4<-separate(cHH9_4,location,c("city","state"),sep=",")

cHH9_4$state<-toupper(cHH9_4$state)
cHH9_4$city<-toupper(cHH9_4$city)

cHH9_4$state<-as.factor(str_replace_all(cHH9_4$state,pattern=" ",replacement=""))
cHH9_4$city<-as.factor(str_replace_all(cHH9_4$city,pattern=" ",replacement=""))

no_state<-filter(cHH9_4,state=="USA"
                 |is.na(state))
y_state<-filter(cHH9_4,state!="USA"
                 &!is.na(state))
```


3.4 I calculated the time each city shows in TX and LA, both in the data that containing a state information and without state information. Add state information to the final dataframe

```{r eval=FALSE}

#remain those have Tx

y_state_1<-filter(y_state,state=="TX"|state=="TEXAS")
y_state_2<-filter(y_state,state=="LA"|state=="LOUISIANA")
y_state_3<-filter(y_state,state=="MA"|state=="MASSACHUSETTS")
y_state_4<-filter(y_state,state=="NV"|state=="NEVADA")
                  
city_c_1<-summary(y_state_1$city,maxsum = 500)
city_c_2<-summary(y_state_2$city,maxsum = 500)
city_c_3<-summary(y_state_3$city,maxsum = 500)
city_c_4<-summary(y_state_4$city,maxsum = 500)
name_city_1<-names(city_c_1)
name_city_2<-names(city_c_2)
name_city_3<-names(city_c_3)
name_city_4<-names(city_c_4)
#change summary of state and city into data frame


city_ystate_1<-data.frame(city_c_1)
city_ystate_2<-data.frame(city_c_2)
city_ystate_3<-data.frame(city_c_3)
city_ystate_4<-data.frame(city_c_4)
city_ystate_1$city<-as.factor(name_city_1)
city_ystate_2$city<-as.factor(name_city_2)
city_ystate_3$city<-as.factor(name_city_3)
city_ystate_4$city<-as.factor(name_city_4)

city_ystate_1$state<-as.factor("TX")
city_ystate_2$state<-as.factor("LA")
city_ystate_3$state<-as.factor("MA")
city_ystate_4$state<-as.factor("NV")

vector<-c("number","city","state")

colnames(city_ystate_1)<-vector
colnames(city_ystate_2)<-vector
colnames(city_ystate_3)<-vector
colnames(city_ystate_4)<-vector

city_ystate<-rbind(city_ystate_1,city_ystate_2,city_ystate_3,city_ystate_4)
#clear nonsense data$rename
city_ystate<-filter(city_ystate,number>0)
#reorder
city_ystate<-city_ystate[order(-city_ystate$number), ]

write.csv(city_ystate,"citys.csv")
```
4.	Graphing:
The graphing process containing several different steps. 
4.1	basic row data exploration. Graphing the first 20 cities that have the most frequency of showing for different dates using ggplot (geom_bar) and for the reference dates, too. The code used showed below, also, take day 9-4 as an example

```{r eval=FALSE}
city_ystate$city <- factor(city_ystate$city, levels=unique(city_ystate$city))
ggplot(head(city_ystate,20),aes(x=city,y=number,fill=state))+
  geom_bar(stat = "identity")+
  coord_flip()+
  labs(title="city count")
```


4.2	using ggmap package and ggplot to graph the area on the map as to visualize the data.
```{r eval=FALSE}
library(ggmap)
library(mapproj)

location<-city_ystate
location$longi<-NA
location$latti<-NA

for(i in 1:nrow(location)){
    location[i, 4:5] <- geocode(as.character(location[i,2])) %>% as.numeric
}

#for those not fitted in the first loop

for(i in 1:nrow(location)){
  if(is.na(location[i, 4])){
  location[i, 4:5] <- geocode(as.character(location[i,2])) %>% as.numeric
  }
  else{}
}
location<-filter(location,!is.na(longi))
location<-filter(location,longi<=-60&longi>=-140&latti>=20&latti<=50)
#compare with the other state:

usa_center <- as.numeric(geocode("United States"))
USAMap <- ggmap(get_googlemap(center=usa_center, zoom=4, maptype = "roadmap"), 
                extent="normal")

USAMap +  geom_point(aes(x=longi, y = latti), data = location,
             alpha=0.6, col="orange",size = location$number*0.1)+
  geom_point(aes(x = longi, y = latti), data = location, 
             alpha = 0.3, col = "blue", size = 1) +
  scale_size_continuous(range = range(location$number))

#compare within the state:

statemap<-get_map(location='Texas',zoom=6,maptype="toner")
statemap<-ggmap(statemap)

texas<-filter(location,state=="TX"|state=="LA")
statemap +  geom_point(aes(x=longi, y = latti), data = texas,
                     alpha=0.2, col="yellow",size = texas$number*0.7)+
  geom_point(aes(x = longi, y = latti), data = texas, 
             alpha = 0.6, col = "red", size = 2) +
  scale_size_continuous(range = range(texas$number))

```

5.	Assumption checking
In order to exclude the influence of the city and total population of the number of tweets, I also pull another day’s data(Sep 30th) as the baseline data, to access whether there is pattern change based on the assumption.


#result:
1.	Raw data:

After the step before, I got the row data for 5 days listed in the table below.
```{r echo=FALSE}
dim<-read.csv("dim.csv")
colnames(dim)[1]<-"dates"
dim
```

2.	Top 20 cities based on different dates

3.	Graphing


```{r echo=FALSE}
city_ystate<-read.csv("citys.csv")
library(ggplot2)
city_ystate$city <- factor(city_ystate$city, levels=unique(city_ystate$city))
ggplot(head(city_ystate,20),aes(x=city,y=number,fill=state))+
  geom_bar(stat = "identity")+
  coord_flip()+
  labs(title="city count")

```
Comparing with other state in the whole map:


```{r echo=FALSE}
library(ggmap)
library(mapproj)
location<-read.csv("location.csv")
#compare with the other state:

usa_center <- as.numeric(geocode("United States"))
USAMap <- ggmap(get_googlemap(center=usa_center, zoom=4, maptype = "roadmap"), 
                extent="normal")

USAMap +  geom_point(aes(x=longi, y = latti), data = location,
             alpha=0.6, col="orange",size = location$number*0.1)+
  geom_point(aes(x = longi, y = latti), data = location, 
             alpha = 0.3, col = "blue", size = 1) +
  scale_size_continuous(range = range(location$number))
```
compare within the state of LA and TX


```{r echo=FALSE}

statemap<-get_map(location='Texas',zoom=6,maptype="toner")
statemap<-ggmap(statemap)

texas<-filter(location,location$state=="TX"|location$state=="LA")
statemap +  geom_point(aes(x=longi, y = latti), data = texas,
                     alpha=0.2, col="yellow",size = texas$number*0.7)+
  geom_point(aes(x = longi, y = latti), data = texas, 
             alpha = 0.6, col = "red", size = 2) +
  scale_size_continuous(range = range(texas$number))

```




4.	In the map




5.	Checking the assumption





# Data source:

* [twitter](https://twitter.com/)
* [twitterR package](http://geoffjentry.hexdump.org/twitteR.pdf)


