---
title: "Hurricane Harvey in America "
author: xueting tao
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# project description

A major disaster is currently underway with [hurricane Harvey](https://en.wikipedia.org/wiki/Hurricane_Harvey.) Use [twitter](https://twitter.com/) to identify times and areas that are hardest hit from the hurricane 

# basic data source

* [twitter](https://twitter.com/)
* [twitterR package](http://geoffjentry.hexdump.org/twitteR.pdf)

#Things to do:


1. start with scripting data from twitter. Since I am not familiar with python, I am going to use TwitterR package to help me. The detail instruction is here http://geoffjentry.hexdump.org/twitteR.pdf & https://github.com/geoffjentry/twitteR

2. Follow the instruction, first I need to create a Twitter Aplication:

I create a new application with the name of xtao8 and get the API key, API secreat, Access token and Access secret.

3. Install twitterR. Since I would like to access Harvey and it mainly start on August 17th at night, I would like to start my search from August 25 and follow until 09/08/2017. 
If the data set is too big, it would be difficult to analysis. So I would like to pull random sample several times and get useful information from them.

The code I used:

```{r eval=FALSE}
#getting the data: install twitterR since install.pacakge("twitterR")is not avialble
#let me try another one
install.packages(c("devtools", "rjson", "bit64", "httr"))
#follow the instruction and restart R
library(devtools)
install_github("geoffjentry/twitteR")

library(twitteR)
setup_twitter_oauth("gLvw86UAzsVGG4lNoBTCttQjg", 
                    "gbBf9OW1PMz2KXMZx9Wo3neKgXNSpvG3abgt3A3TxcBWxefmjm",
                    "770639700839333888-foZXS5jtNXc7bWdpNnUR18Gb5fPQSp9",
                    "n8lL1J7HlAhupOGdOVJFyLcNObxWtyxkffTGAnytyqjY1")
#selection:
1
```




## getting data:(randomly get)

1. First, I would like to try to pull 10000 sample from the twitter randomly and see how much data can be used.
The day I can go back as early as possible is 9/5, so start from 9/5, I will get 5 days data and change it into data frame.

```{r eval=FALSE}
#data at 9-4
sample_9_4<-searchTwitter('#HurricaneHarvey until:2017-9-5', n=10000)
#data at 9-5
sample_9_5<-searchTwitter('#HurricaneHarvey until:2017-9-6', n=10000)
#data at 9-6
sample_9_6<-searchTwitter('#HurricaneHarvey until:2017-9-7', n=10000)
#data at 9-7
sample_9_7<-searchTwitter('#HurricaneHarvey until:2017-9-8', n=10000)
#data at 9-8
sample_9_8<-searchTwitter('#HurricaneHarvey until:2017-9-9', n=10000)
HH9_4<-twListToDF(sample_9_4)
HH9_5<-twListToDF(sample_9_5)
HH9_6<-twListToDF(sample_9_6)
HH9_7<-twListToDF(sample_9_7)
HH9_8<-twListToDF(sample_9_8)
```
clear the useless data
```{r eval=FALSE}
rm(sample_9_4)
rm(sample_9_5)
rm(sample_9_6)
rm(sample_9_7)
rm(sample_9_8)

```

2. Delete lines without location information

```{r eval=FALSE}
#load tidytext and dplyr
library(tidytext)
library(dplyr)

#select data that have location information

HH9_4$location[HH9_4$location==""]<-"NA"  
HH9_4$location<-as.factor(HH9_4$location)
cHH9_4<-filter(HH9_4,location!="NA")
  

HH9_5$location[HH9_5$location==""]<-"NA"  
HH9_5$location<-as.factor(HH9_5$location)
cHH9_5<-filter(HH9_5,location!="NA")


HH9_6$location[HH9_6$location==""]<-"NA"  
HH9_6$location<-as.factor(HH9_6$location)
cHH9_6<-filter(HH9_6,location!="NA")


HH9_7$location[HH9_7$location==""]<-"NA"  
HH9_7$location<-as.factor(HH9_7$location)
cHH9_7<-filter(HH9_7,location!="NA")


HH9_8$location[HH9_8$location==""]<-"NA"  
HH9_8$location<-as.factor(HH9_8$location)
cHH9_8<-filter(HH9_8,location!="NA")

```
EDA report:
After first step cleaning, I got 5datasets with location information.
At the date 9/4, I got `` observations of ` `variables





3. Delete lines with nonsence location information

4. Count the specific location information

5. Delete unrelated location information

6. Put location information into GIS

7. Use IDW tool to draw the difference over time on map.








