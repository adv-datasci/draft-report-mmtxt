---
title: "Hurricane Harvey in America "
author: xueting tao
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# introduction

Hurricane Harvey is an Atlantic hurricane that has attack America for several times in the past year. The most recent attack happened on August 13 when the National Hurricane center detect the hurricane on the western east of Africa. It hit United States on Texas and Louisiana on August 23rd. 
In this project, I am going to use the information from twitter to identify possible the most serious hurt area and its possible moving route.


#prepare for the data:


1. start with scripting data from twitter. Since I am not familiar with python, I am going to use TwitterR package to help me. The detail instruction is here http://geoffjentry.hexdump.org/twitteR.pdf & https://github.com/geoffjentry/twitteR

2. Follow the instruction, first I need to create a Twitter Aplication:

I create a new application with the name of xtao8 and get the API key, API secreat, Access token and Access secret.

3. Install twitterR. Since I would like to access Harvey and it mainly start on August 17th at night, I would like to start my search from August 25 and follow until 09/08/2017. 
If the data set is too big, it would be difficult to analysis. So I would like to pull random sample several times and get useful information from them.

The code I used:

```{r eval=FALSE}
#getting the data: install twitterR since install.pacakge("twitterR")is not avialble
#let me try another one
install.packages(c("devtools", "rjson", "bit64", "httr"))
#follow the instruction and restart R
library(devtools)
install_github("geoffjentry/twitteR")

library(twitteR)
setup_twitter_oauth("gLvw86UAzsVGG4lNoBTCttQjg", 
                    "gbBf9OW1PMz2KXMZx9Wo3neKgXNSpvG3abgt3A3TxcBWxefmjm",
                    "770639700839333888-foZXS5jtNXc7bWdpNnUR18Gb5fPQSp9",
                    "n8lL1J7HlAhupOGdOVJFyLcNObxWtyxkffTGAnytyqjY1")
#selection:
1
```




# getting and cleaning data:

1. First, I would like to try to pull 10000 sample from the twitter randomly and see how much data can be used.
The day I can go back as early as possible is 9/5, so start from 9/5, I will get 5 days data and change it into data frame.

```{r eval=FALSE}
#data at 9-4
sample_9_4<-searchTwitter('#HurricaneHarvey until:2017-9-5', n=10000)
#data at 9-5
sample_9_5<-searchTwitter('#HurricaneHarvey until:2017-9-6', n=10000)
#data at 9-6
sample_9_6<-searchTwitter('#HurricaneHarvey until:2017-9-7', n=10000)
#data at 9-7
sample_9_7<-searchTwitter('#HurricaneHarvey until:2017-9-8', n=10000)
#data at 9-8
sample_9_8<-searchTwitter('#HurricaneHarvey until:2017-9-9', n=10000)
HH9_4<-twListToDF(sample_9_4)
HH9_5<-twListToDF(sample_9_5)
HH9_6<-twListToDF(sample_9_6)
HH9_7<-twListToDF(sample_9_7)
HH9_8<-twListToDF(sample_9_8)
```
clear the useless data
```{r eval=FALSE}
rm(sample_9_4)
rm(sample_9_5)
rm(sample_9_6)
rm(sample_9_7)
rm(sample_9_8)

```

2. Delete lines without location information

```{r eval=FALSE}
#read in the dataset



#load tidytext and dplyr
library(tidytext)
library(dplyr)

#select data that have location information

HH9_4$location[HH9_4$location==""]<-"NA"  
HH9_4$location<-as.factor(HH9_4$location)
cHH9_4<-filter(HH9_4,location!="NA")
  

HH9_5$location[HH9_5$location==""]<-"NA"  
HH9_5$location<-as.factor(HH9_5$location)
cHH9_5<-filter(HH9_5,location!="NA")


HH9_6$location[HH9_6$location==""]<-"NA"  
HH9_6$location<-as.factor(HH9_6$location)
cHH9_6<-filter(HH9_6,location!="NA")


HH9_7$location[HH9_7$location==""]<-"NA"  
HH9_7$location<-as.factor(HH9_7$location)
cHH9_7<-filter(HH9_7,location!="NA")


HH9_8$location[HH9_8$location==""]<-"NA"  
HH9_8$location<-as.factor(HH9_8$location)
cHH9_8<-filter(HH9_8,location!="NA")

```
#basic EDA report:
```{r echo=FALSE}
dim<-read.csv("dim.csv")
colnames(dim)[1]<-"dates"
dim
```





#Basic cleaning of the data(9/4 as an example)


```{r eval=FALSE}

cHH9_4<-separate(cHH9_4,location,c("city","state"),sep=",")

cHH9_4$state<-toupper(cHH9_4$state)
cHH9_4$city<-toupper(cHH9_4$city)

cHH9_4$state<-as.factor(str_replace_all(cHH9_4$state,pattern=" ",replacement=""))
cHH9_4$city<-as.factor(str_replace_all(cHH9_4$city,pattern=" ",replacement=""))

no_state<-filter(cHH9_4,state=="USA"
                 |is.na(state))
y_state<-filter(cHH9_4,state!="USA"
                 &!is.na(state))
```


# cleaning_continue

```{r eval=FALSE}

#remain those have Tx

state_num<-summary(y_state$state,maxsum = 500)

y_state<-filter(y_state,state=="TX"|state=="TEXAS")
city_c<-summary(y_state$city,maxsum = 500)
name_city<-names(city_c)

#change summary of state and city into data frame

city_ystate<-data.frame(city_c)
city_ystate$city<-as.factor(name_city)

#clear nonsense data$rename
city_ystate<-filter(city_ystate,city_c>0)
vector<-c("number","city")
citylist<-city_ystate$city

colnames(city_ystate)<-vector
rownames(city_ystate)<-citylist

#for those without state
#use citylist to find same city and add to count

city_nstate<-no_state[ ,"city"]
city_nstate<-


#bind city together:
city_total<-city_ystate+city_nstate


```


# match city to geographic points


# Put location information into GIS

# Use IDW tool to draw the difference over time on map.





# Data source:

* [twitter](https://twitter.com/)
* [twitterR package](http://geoffjentry.hexdump.org/twitteR.pdf)


