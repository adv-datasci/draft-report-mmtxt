---
title: "Hurricane Harvey in America "
author: xueting tao
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# project description

A major disaster is currently underway with [hurricane Harvey](https://en.wikipedia.org/wiki/Hurricane_Harvey.) Use [twitter](https://twitter.com/) to identify times and areas that are hardest hit from the hurricane 

# basic data source

* [twitter](https://twitter.com/)
* [twitterR package](http://geoffjentry.hexdump.org/twitteR.pdf)

#Things to do:


1. start with scripting data from twitter. Since I am not familiar with python, I am going to use TwitterR package to help me. The detail instruction is here http://geoffjentry.hexdump.org/twitteR.pdf & https://github.com/geoffjentry/twitteR

2. Follow the instruction, first I need to create a Twitter Aplication:

I create a new application with the name of xtao8 and get the API key, API secreat, Access token and Access secret.

3. Install twitterR. Since I would like to access Harvey and it mainly start on August 17th at night, I would like to start my search from August 25 and follow until 09/08/2017. 
If the data set is too big, it would be difficult to analysis. So I would like to pull random sample several times and get useful information from them.

The code I used:

```{r eval=FALSE}
#getting the data: install twitterR since install.pacakge("twitterR")is not avialble
#let me try another one
install.packages(c("devtools", "rjson", "bit64", "httr"))
#follow the instruction and restart R
library(devtools)
install_github("geoffjentry/twitteR")

library(twitteR)
setup_twitter_oauth("gLvw86UAzsVGG4lNoBTCttQjg", 
                    "gbBf9OW1PMz2KXMZx9Wo3neKgXNSpvG3abgt3A3TxcBWxefmjm",
                    "770639700839333888-foZXS5jtNXc7bWdpNnUR18Gb5fPQSp9",
                    "n8lL1J7HlAhupOGdOVJFyLcNObxWtyxkffTGAnytyqjY1")
#selection:
1
```




## getting and cleaning data:(randomly get)

1. First, I would like to try to pull 10000 sample from the twitter randomly and see how much data can be used.
The day I can go back as early as possible is 9/5, so start from 9/5, I will get 5 days data and change it into data frame.

```{r eval=FALSE}
#data at 9-4
sample_9_4<-searchTwitter('#HurricaneHarvey until:2017-9-5', n=10000)
#data at 9-5
sample_9_5<-searchTwitter('#HurricaneHarvey until:2017-9-6', n=10000)
#data at 9-6
sample_9_6<-searchTwitter('#HurricaneHarvey until:2017-9-7', n=10000)
#data at 9-7
sample_9_7<-searchTwitter('#HurricaneHarvey until:2017-9-8', n=10000)
#data at 9-8
sample_9_8<-searchTwitter('#HurricaneHarvey until:2017-9-9', n=10000)
HH9_4<-twListToDF(sample_9_4)
HH9_5<-twListToDF(sample_9_5)
HH9_6<-twListToDF(sample_9_6)
HH9_7<-twListToDF(sample_9_7)
HH9_8<-twListToDF(sample_9_8)
```
clear the useless data
```{r eval=FALSE}
rm(sample_9_4)
rm(sample_9_5)
rm(sample_9_6)
rm(sample_9_7)
rm(sample_9_8)

```

2. Delete lines without location information

```{r eval=FALSE}
#read in the dataset



#load tidytext and dplyr
library(tidytext)
library(dplyr)

#select data that have location information

HH9_4$location[HH9_4$location==""]<-"NA"  
HH9_4$location<-as.factor(HH9_4$location)
cHH9_4<-filter(HH9_4,location!="NA")
  

HH9_5$location[HH9_5$location==""]<-"NA"  
HH9_5$location<-as.factor(HH9_5$location)
cHH9_5<-filter(HH9_5,location!="NA")


HH9_6$location[HH9_6$location==""]<-"NA"  
HH9_6$location<-as.factor(HH9_6$location)
cHH9_6<-filter(HH9_6,location!="NA")


HH9_7$location[HH9_7$location==""]<-"NA"  
HH9_7$location<-as.factor(HH9_7$location)
cHH9_7<-filter(HH9_7,location!="NA")


HH9_8$location[HH9_8$location==""]<-"NA"  
HH9_8$location<-as.factor(HH9_8$location)
cHH9_8<-filter(HH9_8,location!="NA")

```
### EDA report:

After first step cleaning, I got 5datasets with location information.
At the date 9/4, I got `dim(cHH9_4)[1]` observations of `dim(cHH9_4)[2]`variables
At the date 9/5, I got `dim(cHH9_5)[1]` observations of `dim(cHH9_5)[2] `variables
At the date 9/6, I got `dim(cHH9_6)[1]` observations of `dim(cHH9_6)[2] `variables
At the date 9/7, I got `dim(cHH9_7)[1]` observations of `dim(cHH9_7)[2] `variables
At the date 9/8, I got `dim(cHH9_8)[1]` observations of `dim(cHH9_8)[2] `variables



3. Basic cleaning of the data

# seperete state and city $devided data into those have states and don't.(9/4 as an example)

```{r eval=FALSE}

cHH9_4<-separate(cHH9_4,location,c("city","state"),sep=",")

cHH9_4$state<-toupper(cHH9_4$state)
cHH9_4$city<-toupper(cHH9_4$city)

cHH9_4$state<-as.factor(str_replace_all(cHH9_4$state,pattern=" ",replacement=""))
cHH9_4$city<-as.factor(str_replace_all(cHH9_4$city,pattern=" ",replacement=""))

cHH9_4$city<-as.factor(str_replace_all(cHH9_4$city,pattern=" ",replacement=""))

no_state<-filter(cHH9_4,state=="USA"
                 |is.na(state))
y_state<-filter(cHH9_4,state!="USA"
                 &!is.na(state))
```


# basic cleaningn

```{r eval=FALSE}

#remain those have Tx

y_state_t<-y_state
state_num<-summary(y_state$state,maxsum = 500)

y_state_t<-filter(y_state_t,state=="TX"|state=="TEXAS")
city_c<-summary(y_state_t$city,maxsum = 500)
name_city<-names(city_c)

#change summary of state and city into data frame

city_ystate<-data.frame(city_c)
city_ystate$city<-as.factor(name_city)

#clear nonsense data$rename
city_ystate<-filter(city_ystate,city_c>0)
vector<-c("number","city")
citylist<-city_ystate$city

colnames(city_ystate)<-vector
rownames(city_ystate)<-citylist

#for those without state


#bind city together:
city_total<-city_ystate+city_nstate


```


4. Count the specific location information






5. Put location information into GIS

6. Use IDW tool to draw the difference over time on map.








